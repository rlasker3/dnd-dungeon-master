{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f727121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlasker/ml_projects/dnd_ai_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# What device will we run on?\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# We'll use a small model first to see mechanics clearly\n",
    "model_name = \"microsoft/phi-2\"  # 2.7B params, ~6GB, fast on your hardware\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8c8a17-3cea-4059-9ab4-90bf09de379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/phi-2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:50<00:00, 25.48s/it]\n",
      "Loading weights: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 453/453 [00:00<00:00, 2384.04it/s, Materializing param=model.layers.31.self_attn.v_proj.weight]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on mps\n",
      "Parameters: 2.78B\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading {model_name}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # half precision - why?\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7f7c61-4910-43c9-88f9-957270c54b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs: tensor([[  464,  2151, 14170,   257,  3223, 18974,    13,   383, 18974,  4958,\n",
      "          8477,    25]], device='mps:0')\n",
      "Token count: 12\n",
      "Tokens decoded individually:\n",
      "  464 → 'The'\n",
      "  2151 → ' party'\n",
      "  14170 → ' enters'\n",
      "  257 → ' a'\n",
      "  3223 → ' dark'\n",
      "  18974 → ' dungeon'\n",
      "  13 → '.'\n",
      "  383 → ' The'\n",
      "  18974 → ' dungeon'\n",
      "  4958 → ' master'\n",
      "  8477 → ' describes'\n",
      "  25 → ':'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The party enters a dark dungeon. The dungeon master describes:\"\n",
    "\n",
    "# Tokenize - this is what the model actually sees\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "print(f\"Input token IDs: {inputs['input_ids']}\")\n",
    "print(f\"Token count: {inputs['input_ids'].shape[1]}\")\n",
    "print(f\"Tokens decoded individually:\")\n",
    "for token_id in inputs['input_ids'][0]:\n",
    "    print(f\"  {token_id.item()} → '{tokenizer.decode([token_id])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6578d0da-7ed3-41f9-a3d5-1cb765e3f95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New token IDs: tensor([  921,  3802,   257,  3223,   290, 21151, 18974,    13,   383,  1633,\n",
      "          318,  6546,   351,   262,  8508,   286, 22119,   290, 15936,    13,\n",
      "          921,  3285,   262,  2128,   286, 37472,  1660,   290, 18107, 30346,\n",
      "          286,  6941,   504,   290,  7128,   504,    13,   921,   766,   257,\n",
      "        18107,  1657,   287,   262,  5253,    11,  2406,   422,   257,  1588,\n",
      "         3420,   379,   262,   886,   286,   262, 20749,    13,  2141,   345,\n",
      "          765,   284,    25,   317,     8,  6062,   262,  3420,   290,   766,\n",
      "          644,   338,  2641,    13,   347,     8, 41401,   262, 18974,  2252,\n",
      "           13,   327,     8,  6756,   736,   290,   804,   329,  1194,   835,\n",
      "          503,    13,   198, 15037, 45532,    25,   317,     8,  6062,   262],\n",
      "       device='mps:0')\n",
      "\n",
      "Generated text:\n",
      " You enter a dark and damp dungeon. The air is thick with the smell of decay and mold. You hear the sound of dripping water and faint echoes of moans and groans. You see a faint light in the distance, coming from a large door at the end of the corridor. Do you want to: A) Enter the door and see what's inside. B) Explore the dungeon further. C) Turn back and look for another way out.\n",
      "ANSWER: A) Enter the\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.8,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "# Decode just the new tokens\n",
    "new_tokens = output[0][inputs['input_ids'].shape[1]:]\n",
    "print(f\"New token IDs: {new_tokens}\")\n",
    "print(f\"\\nGenerated text:\")\n",
    "print(tokenizer.decode(new_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a895b0e-a7b4-4045-9034-07067378fb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:47<00:00, 35.89s/it]\n",
      "Loading weights: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 291/291 [00:00<00:00, 297.45it/s, Materializing param=model.norm.weight]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 7.24B\n"
     ]
    }
   ],
   "source": [
    "# Clear previous model from GPU memory first\n",
    "del model\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "model_name = \"microsoft/phi-2\"  # we'll switch to instruct variant\n",
    "# Actually let's use a proper instruct model\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c125490-4088-4127-adc2-7d333b8555f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 49\n",
      "Nestled deep within the heart of the Misty Mountains, the ancient dwarven fortress of Thundertop is shrouded in an aura of mystery and intrigue. The entrance, a massive, ornately carved stone door, is hidden behind a waterfall, its thunderous roar masking the sounds of the fortress within. The air is thick with the scent of damp moss and the faint, metallic tang of iron, while the dim, flickering light of moss-covered lanterns cast eerie, dancing shadows upon the damp, moss-covered walls. As adventurers approach, they cannot help but feel a sense of awe and trepidation, knowing that they are\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"<s>[INST] You are a Dungeon Master for a D&D campaign. \n",
    "Describe the entrance to an ancient dwarven fortress in 3 sentences. \n",
    "Be atmospheric and specific. [/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "print(f\"Input tokens: {inputs['input_ids'].shape[1]}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.8,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "new_tokens = output[0][inputs['input_ids'].shape[1]:]\n",
    "print(tokenizer.decode(new_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633207f2-6f91-43c3-b7ae-b2ea7e3d670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nestled deep within the heart of the Frostfell Mountains, the entrance to the ancient dwarven fortress of Thundertop is concealed by a cascading waterfall, its thundering roar masking the faint hum of magical wards that keep intruders at bay. Carved from living stone, the imposing iron-bound doors bear the emblem of the Stonehammer Clan, with runes that pulse with an inner light, guarding the threshold to the realm of the stout-hearted dwarves. </description>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"<s>[INST] You are a Dungeon Master for a D&D campaign. \n",
    "Describe the entrance to an ancient dwarven fortress in exactly 3 sentences.\n",
    "End your response with </description> [/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.8,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        stop_strings=[\"</description>\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "new_tokens = output[0][inputs['input_ids'].shape[1]:]\n",
    "print(tokenizer.decode(new_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc51d91d-b117-4155-a671-dade02e77040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory freed\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.mps.empty_cache()\n",
    "print(\"GPU memory freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d6ae82-3c58-4a7f-ad61-42bdc3b1d553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
